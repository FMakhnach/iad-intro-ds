{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание №7: деревья, случайный лес (14.5 баллов)\n",
    "<!-- ![](meme.jpg) -->\n",
    "<img src=\"meme.jpg\" alt=\"Drawing\" style=\"width: 700px;\"/>\n",
    "\n",
    "## Часть 1: основы построения решающие дерева\n",
    "\n",
    "начнём с простого..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.1 (1 балл)\n",
    "Пусть известно, что в вершину решающего дерева попали 10 объектов, 8 из которых имеют метку класса $k_1$, а 2 - $k_2$. Посчитайте энтропию (с натуральным логарифмом). Ответ округлите до двух знаков после запятой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (*・‿・)ノ⌒*:･ﾟ✧"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.50'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from math import log\n",
    "\n",
    "k = [8, 2]\n",
    "sum_k = sum(k)\n",
    "p = [x / sum_k for x in k]\n",
    "\n",
    "result = -sum([p_i * log(p_i) for p_i in p])\n",
    "format(result, '.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.2 (1 балл)\n",
    "Пусть дополнительно известно, что вершина из предыдущего задания не является листовой и возможно такое разбиение, что в левое поддерево попадут все объекты класса $k_1$, а в правое - класса $k_2$. Посчитайте критерий информативности (в формулировке разности impurity пришедшей выборки и полученного разбиения) такой вершины с применением индекса Джини. Ответ округлите до двух знаков после запятой."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (￣▽￣)/♫•*¨*•.¸¸♪"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(arr):\n",
    "    sum_arr = sum(arr)\n",
    "    p = [x / sum_arr for x in arr]\n",
    "    \n",
    "    return sum([p_i * (1 - p_i) for p_i in p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = gini([8, 2]) - gini([8]) - gini([2])\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1.3 (0.5 балла)\n",
    "Пусть при построении дерева образовалась листовая вершина с 10 объектами, значения целевой переменной для которых следующие: [1, 10, 5, 18, 100, 30, 50, 61, 84, 47] (решается задача регрессии). Чему будут равны предсказания модели для этих объектов?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ༼ つ ◕_◕ ༽つ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40.6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# По идее берём среднее, но по факту нам никто не запрещает использовать другое правило (например, брать медиану)\n",
    "r = [1, 10, 5, 18, 100, 30, 50, 61, 84, 47]\n",
    "sum(r) / len(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2: решающие деревья\n",
    "\n",
    "### Задание 2.1 (2 балла)\n",
    "Первым делом реализуйте функцию `find_best_split`, которая должна находить оптимальное разбиение подмножества обучающей выборки. При решении задачи регрессии испольуйте дисперсию подвыборки, при решении задачи классификации используйте критерий Джини. Эту функцию можно протестировать на датасете `Boston` из `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _boston_dataset:\n",
      "\n",
      "Boston house prices dataset\n",
      "---------------------------\n",
      "\n",
      "**Data Set Characteristics:**  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive. Median Value (attribute 14) is usually the target.\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/housing/\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      ".. topic:: References\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "boston = load_boston()\n",
    "X = pd.DataFrame(data=boston[\"data\"], columns=boston[\"feature_names\"])\n",
    "y = boston[\"target\"]\n",
    "X.head()\n",
    "\n",
    "print(boston[\"DESCR\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При решении задания имейте в виду, что под критерием Джини подразумевается такая функция:\n",
    "\n",
    "$$Q(R) = -\\frac {|R_{\\ell}|}{|R|}H(R_\\ell) -\\frac {|R_r|}{|R|}H(R_r) ,$$\n",
    "\n",
    "где $R$ — множество объектов, попавших в вершину, $R_{\\ell}$ и $R_r$ — объекты, попавшие в левое и правое поддеревья,\n",
    "$H(R) = 1 - p_1^2 - p_0^2$, $p_1$, $p_0$ — доли объектов класса 1 и 0 соответственно.\n",
    "\n",
    "Для категориальных признаков применяется наивный алгоритм разбиения: мы пытаемся найти одно значение, разбиение по которому сильнее всего увеличит критерий информативности. Иными словами, объекты с конкретным значением признака отправляем в левое поддерево, остальные - в правое. Обратите внимание, что это далеко не оптимальные способ учёта категориальных признаков. Например, можно было бы на каждое значение категориального признака создавать отдельное поддерево или использовать более сложные подходы. Подробнее об этом можно прочитать в конспектах [лекций](https://github.com/esokolov/ml-course-hse/blob/master/2019-fall/lecture-notes/lecture07-trees.pdf) по машинному обучению на ПМИ (раздел «Учёт категориальных признаков»).\n",
    "\n",
    "В качестве подсказок реализации можете пользоваться кодом из бонусной части семинара по решающим деревьям (скачайте решённую версию ноутбука).\n",
    "\n",
    "**Note:** Разрешается делать цикл для перебора порогов, но возможна имплементация без него. За имплементацию без цикла бонус 0.7 балла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(values: np.ndarray, feature_type: str) -> float:\n",
    "    if feature_type == 'real':\n",
    "        return values.var()\n",
    "    elif feature_type == 'categorical':\n",
    "        _, counts = numpy.unique(values, return_counts=True)\n",
    "        sum_values = sum(values)\n",
    "        frequences = counts.apply(lambda x: x / sum_values)\n",
    "        return sum(frequences.apply(lambda p: p * (1 - p)))\n",
    "    else:\n",
    "        raise Exception(f'Unexpected feature type {feature_type}')\n",
    "\n",
    "\n",
    "def split_node(\n",
    "    feature_vector: np.ndarray,\n",
    "    target_vector: np.ndarray, \n",
    "    feature_type: str,\n",
    "    t: float\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    if feature_type == 'real':\n",
    "        mask = feature_vector <= t\n",
    "        return feature_vector[mask], feature_vector[~mask]\n",
    "    elif feature_type == 'categorical':\n",
    "        mask = feature_vector == t\n",
    "        return feature_vector[mask], feature_vector[~mask]\n",
    "    else:\n",
    "        raise Exception(f'Unexpected feature type {feature_type}')\n",
    "\n",
    "\n",
    "def gini(\n",
    "    feature_vector: Union[np.ndarray, pd.DataFrame],\n",
    "    target_vector: Union[np.ndarray, pd.Series],\n",
    "    feature_type: str,\n",
    "    t\n",
    ") -> float:\n",
    "    R_l, R_r = split_node(feature_vector, target_vector, feature_type, t)\n",
    "    return len(R_l) / len(feature_vector) * H(R_l, feature_type) + len(R_r) / len(feature_vector) * H(R_r, feature_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_split(\n",
    "    feature_vector: Union[np.ndarray, pd.DataFrame], \n",
    "    target_vector: Union[np.ndarray, pd.Series],\n",
    "    task: str = \"classification\",\n",
    "    feature_type: str = \"real\"\n",
    ") -> Tuple[np.ndarray, np.ndarray, float, float]:\n",
    "    \"\"\"\n",
    "    Указания:\n",
    "    * Пороги, приводящие к попаданию в одно из поддеревьев пустого множества объектов, не рассматриваются.\n",
    "    * В качестве порогов, нужно брать среднее двух соседних (при сортировке) значений признака\n",
    "    * Поведение функции в случае константного признака может быть любым.\n",
    "    * При одинаковых приростах Джини или дисперсии нужно выбирать минимальный сплит.\n",
    "    * За наличие в функции циклов балл будет снижен. Векторизуйте! :)\n",
    "\n",
    "    :param feature_vector: вещественнозначный вектор значений признака\n",
    "    :param target_vector: вектор классов объектов,  len(feature_vector) == len(target_vector)\n",
    "    :param task: либо `classification`, либо `regression`\n",
    "    :param feature_type: либо `real`, либо `categorical`\n",
    "    \n",
    "    :return thresholds: отсортированный по возрастанию вектор со всеми возможными порогами, по которым объекты можно\n",
    "     разделить на две различные подвыборки, или поддерева\n",
    "    :return ginis: вектор со значениями критерия Джини для каждого из порогов в thresholds len(ginis) == len(thresholds)\n",
    "    :return threshold_best: оптимальный порог (число)\n",
    "    :return gini_best: оптимальное значение критерия Джини (число)\n",
    "    \"\"\"\n",
    "    # ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-\n",
    "    thresholds = np.sort(np.unique(target_vector))\n",
    "    ginis = [gini(feature_vector, target_vector, feature_type, t) for t in thresholds]\n",
    "    ginis = np.nan_to_num(ginis, nan=float(\"+inf\"))\n",
    "    \n",
    "    threshold_best = thresholds[np.argmin(ginis)]\n",
    "    gini_best = min(ginis)\n",
    "    \n",
    "    return thresholds, ginis, threshold_best, gini_best\n",
    "\n",
    "thresholds, variances, threshold_best, variance_best = find_best_split(\n",
    "    X[\"CRIM\"].to_numpy(), \n",
    "    y, \n",
    "    task=\"regression\",\n",
    "    feature_type=\"real\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведите график зависимости значения критерия ошибки от порогового значения при разбиении вершины по признаку `CRIM`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(thresholds, variances)\n",
    "plt.title(\"keke\")\n",
    "plt.xlabel(\"threshold\")\n",
    "plt.ylabel(\"Q error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.2 (3 балла)\n",
    "Разберитесь с написанным кодом решающего дерева, заполните пропуски в коде и реализуйте недостающий метод _predict_node.\n",
    "\n",
    "Построение дерева осуществляется согласно базовому жадному алгоритму, предложенному в лекции в разделе «Построение дерева».\n",
    "- Выбор лучшего разбиения необходимо производить по критерию Джини\n",
    "- Критерий останова: все объекты в листе относятся к одному классу или ни по одному признаку нельзя разбить выборку\n",
    "- Ответ в листе: наиболее часто встречающийся класс в листе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTree:\n",
    "    def __init__(\n",
    "        self, \n",
    "        feature_types: Union[List[str], np.ndarray], \n",
    "        max_depth: int = None, \n",
    "        min_samples_split: int = None, \n",
    "        min_samples_leaf: int = None,\n",
    "        task: str = \"classification\"\n",
    "    ) -> None:\n",
    "        \n",
    "        if np.any(list(map(lambda x: x != \"real\" and x != \"categorical\", feature_types))):\n",
    "            raise ValueError(\"There is unknown feature type\")\n",
    "\n",
    "        # В этой переменной будем хранить узлы решающего дерева. Каждая вершина хранит в себе идентификатор того,\n",
    "        # является ли она листовой. Листовые вершины хранят значение класса для предсказания, нелистовые - правого и\n",
    "        # левого детей (поддеревья для продолжения процедуры предсказания)\n",
    "        self._tree = {}\n",
    "        \n",
    "        # типы признаков (категориальные или числовые)\n",
    "        self._feature_types = feature_types\n",
    "        \n",
    "        # гиперпараметры дерева\n",
    "        self._max_depth = max_depth\n",
    "        self._min_samples_split = min_samples_split\n",
    "        self._min_samples_leaf = min_samples_leaf\n",
    "        self.task = task\n",
    "\n",
    "    def _fit_node(\n",
    "        self, \n",
    "        sub_X: np.ndarray, \n",
    "        sub_y: np.ndarray, \n",
    "        node: dict\n",
    "    ) -> None:\n",
    "        \n",
    "        # критерий останова\n",
    "        if np.all(sub_y == sub_y[0]):\n",
    "            node[\"type\"] = \"terminal\"\n",
    "            node[\"class\"] = sub_y[0]\n",
    "            return\n",
    "\n",
    "        feature_best, threshold_best, gini_best, split = None, None, None, None\n",
    "        for feature in range(sub_X.shape[1]):\n",
    "            feature_type = self._feature_types[feature]\n",
    "            categories_map = {}\n",
    "\n",
    "            # подготавливаем признак для поиска оптимального порога\n",
    "            if feature_type == \"real\":\n",
    "                feature_vector = sub_X[:, feature]\n",
    "            elif feature_type == \"categorical\":\n",
    "                # здесь могла быть реализация более сложного подхода к обработке категориального признака\n",
    "                feature_vector = sub_X[:, feature]\n",
    "\n",
    "            # ищем оптимальный порог\n",
    "            _, _, threshold, gini = find_best_split(feature_vector, sub_y, self.task, feature_type)\n",
    "            \n",
    "            if gini_best is None or gini > gini_best:\n",
    "                feature_best = feature\n",
    "                gini_best = gini\n",
    "\n",
    "                # split - маска на объекты, которые должны попасть в левое поддерево\n",
    "                if feature_type == \"real\":\n",
    "                    threshold_best = threshold\n",
    "                    split = sub_X <= threshold_best # ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-\n",
    "                elif feature_type == \"categorical\":\n",
    "                    # в данной реализации это просто значение категории\n",
    "                    threshold_best = threshold\n",
    "                    split = sub_X == threshold_best # ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-\n",
    "                else:\n",
    "                    raise ValueError\n",
    "\n",
    "        # записываем полученные сплиты в атрибуты класса\n",
    "        if feature_best is None:\n",
    "            node[\"type\"] = \"terminal\"\n",
    "            node[\"class\"] = Counter(sub_y).most_common(1)[0][0]\n",
    "            return\n",
    "\n",
    "        node[\"type\"] = \"nonterminal\"\n",
    "\n",
    "        node[\"feature_split\"] = feature_best\n",
    "        if self._feature_types[feature_best] == \"real\":\n",
    "            node[\"threshold\"] = threshold_best\n",
    "        elif self._feature_types[feature_best] == \"categorical\":\n",
    "            node[\"category_split\"] = threshold_best\n",
    "        else:\n",
    "            raise ValueError\n",
    "            \n",
    "        node[\"left_child\"], node[\"right_child\"] = {}, {}\n",
    "        self._fit_node(sub_X[split], sub_y[split], node[\"left_child\"])\n",
    "        self._fit_node(sub_X[np.logical_not(split)], sub_y[np.logical_not(split)], node[\"right_child\"])\n",
    "\n",
    "    def _predict_node(self, x: np.ndarray, node: dict) -> int:\n",
    "        \"\"\"\n",
    "        Предсказание начинается с корневой вершины дерева и рекурсивно идёт в левое или правое поддерево в зависимости от значения\n",
    "        предиката на объекте. Листовая вершина возвращает предсказание.\n",
    "        :param x: np.array, элемент выборки\n",
    "        :param node: dict, вершина дерева\n",
    "        \"\"\"\n",
    "        if node[\"type\"] == \"terminal\":\n",
    "            \n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> None:\n",
    "        self._fit_node(X, y, self._tree)\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        predicted = []\n",
    "        for x in X:\n",
    "            predicted.append(self._predict_node(x, self._tree))\n",
    "            \n",
    "        return np.array(predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.3 (1 балл)\n",
    "Загрузите таблицу [students.csv](https://drive.google.com/file/d/0B2zoFVYw1rN3a0d0Zm43TzQ4aUU/view?usp=sharing) (это немного преобразованный датасет [User Knowledge](https://archive.ics.uci.edu/ml/datasets/User+Knowledge+Modeling)). В ней признаки объекта записаны в первых пяти столбцах, а в последнем записана целевая переменная (класс: 0 или 1). Постройте на одном изображении пять кривых \"порог — значение критерия Джини\" для всех пяти признаков. Отдельно визуализируйте scatter-графики \"значение признака — класс\" для всех пяти признаков."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.4 (1 балл)\n",
    "\n",
    "Исходя из кривых значений критерия Джини, по какому признаку нужно производить деление выборки на два поддерева? Согласуется ли этот результат с визуальной оценкой scatter-графиков? Как бы охарактеризовали вид кривой для \"хороших\" признаков, по которым выборка делится почти идеально? Чем отличаются кривые для признаков, по которым деление практически невозможно?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2.5 (1 балл)\n",
    "\n",
    "Протестируйте свое решающее дерево на датасете [mushrooms](https://archive.ics.uci.edu/ml/datasets/Mushroom). Вам нужно скачать таблицу agaricus-lepiota.data (из [Data Folder](https://archive.ics.uci.edu/ml/machine-learning-databases/mushroom/)), прочитать ее с помощью pandas, применить к каждому столбцу LabelEncoder (из sklearn), чтобы преобразовать строковые имена категорий в натуральные числа. Первый столбец — это целевая переменная (e — edible, p — poisonous) Мы будем измерять качество с помощью accuracy, так что нам не очень важно, что будет классом 1, а что — классом 0. Обучите решающее дерево на половине случайно выбранных объектов (признаки в датасете категориальные) и сделайте предсказания для оставшейся половины. Вычислите accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3: бэггинг, случайный лес"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данной части будем работать [с задачей предсказания диабета у пациента](https://www.kaggle.com/uciml/pima-indians-diabetes-database/data). Посмотрим на работу бэггинга над решающими деревьями и случайного леса, сравним их работу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('diabetes.csv')\n",
    "print(f\"Dataset shape: {data.shape}\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на распределение целевой переменной"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Outcome'].hist()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделите данные на признаки и целевую переменную. Разбейте датасет на тренировочную и тестовую части в отношении 70:30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.1 (1 балл)\n",
    "\n",
    "Обучите [`BaggingClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html) на 50 деревьях. Оцените качество классификации на тестовой выборке по метрикам `accuracy`, `precision` и `recall`, `auc_roc`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.2 (1 балл)\n",
    "\n",
    "Теперь обучите Random Forest с таким же количеством деревьев. Оцените качество классификации по тем же метрикам. Какая из двух построенных моделей показала себя лучше?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.3 (1 балл)\n",
    "\n",
    "Для случайного леса проанализируйте значение AUC-ROC на этих же данных в зависимости от изменения параметров (можете сделать обычный перебор с обучением/тестированием в цикле):\n",
    "* `'n_estimators'` (можно перебрать около 10 значений из отрезка от 10 до 1500)\n",
    "* `'min_samples_leaf'` (сетку значений можете выбрать на ваше усмотрение)\n",
    "\n",
    "Постройте соответствующие графики зависимости AUC-ROC от этих параметров. Используйте heat map для визуализации. Какие выводы вы можете сделать?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3.4 (1 балл)\n",
    "\n",
    "Для лучшей модели случайного леса посчитайте важность признаков и постройте bar plot. Какой признак оказался самым важным для определения диабета?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ᕕ(╭ರ╭ ͟ʖ╮•́)⊃¤=(————-"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
